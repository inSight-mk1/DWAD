# DWAD 数据更新策略说明

## 背景

### 前复权数据的特性

DWAD系统使用**前复权**（ADJUST_PREV）数据计算指数，这种复权方式有以下特点：

1. **动态调整**：以当前最新价格为基准，向前调整所有历史价格
2. **除权影响**：当发生分红、送股等除权除息事件时，除权日之前的所有历史价格都会被重新计算
3. **时间依赖**：不同时刻调用API获取的历史数据，使用的复权因子不同

### 数据连续性问题

**问题场景**：

假设某股票在2025-10-20发生10送10除权事件：

| 日期 | 不复权价格 | 10-19下载的前复权 | 10-21下载的前复权 | 增量更新后的数据 |
|------|-----------|------------------|------------------|-----------------|
| 10-18 | 100元 | 100元 | **50元** | **100元** ❌ |
| 10-19 | 102元 | 102元 | **51元** | **102元** ❌ |
| 10-20 | 51元 | - | 51元 | **51元** ✓ |
| 10-21 | 52元 | - | 52元 | **52元** ✓ |

**结果**：10-19到10-20看起来暴跌50%，但实际只是除权！这会导致：
- ❌ 指数计算出现虚假的暴涨/暴跌
- ❌ 排名严重失真
- ❌ 历史回测结果不可信

## 解决方案：全量更新策略

### 策略说明

每次更新数据时，采用**全量刷新**策略：

1. **删除所有现有股票数据**
2. **重新下载从起始日期到今天的全部历史数据**
3. **确保所有数据使用统一的复权基准（当前时刻）**

### 优缺点

**优点**：
- ✅ 数据完全连续，无不一致问题
- ✅ 所有数据使用统一的复权基准
- ✅ 指数计算准确可靠
- ✅ 实现简单，不易出错

**缺点**：
- ⚠️ 每次更新耗时较长（约10-20分钟，取决于股票数量）
- ⚠️ API调用量大
- ⚠️ 需要稳定的网络连接

## 使用方法

### 自动模式（推荐）

直接运行数据下载脚本，系统会自动判断：

```bash
python scripts/download_data.py
```

- **首次运行**：下载所有股票的全部历史数据
- **已有数据**：删除旧数据，重新下载全部历史数据（全量更新）

### 配置文件模式

在 `config/config.yaml` 中设置 `data_fetcher.mode`：

```yaml
data_fetcher:
  mode: auto  # 可选值：auto, initial, update, refresh
  default_start_date: "2020-01-01"
  batch_size: 50
  resume_download: true
```

**模式说明**：

- **`auto`**（默认）：自动判断，首次下载或全量更新
- **`initial`**：强制初始下载所有股票
- **`update`**：强制全量更新（删除旧数据并重新下载）
- **`refresh`**：同 `update`，明确指定全量刷新

### 手动调用

在Python代码中使用：

```python
from dwad.tools.data_downloader import DataDownloader

downloader = DataDownloader()

# 全量刷新数据
success = downloader.full_refresh_data()

if success:
    print("数据更新成功")
else:
    print("数据更新失败，请检查日志")
```

## 更新频率建议

### 日常使用

- **推荐频率**：每天收盘后运行一次
- **执行时间**：晚上或周末（避开交易时段）
- **自动化**：可以设置定时任务

### Windows定时任务示例

1. 打开"任务计划程序"
2. 创建基本任务
3. 设置触发器：每天 17:00（收盘后）
4. 操作：启动程序
   - 程序：`python`
   - 参数：`scripts/download_data.py`
   - 起始位置：`D:\projects\DWAD`

### Linux/Mac定时任务示例

编辑crontab：
```bash
crontab -e
```

添加任务：
```bash
# 每天17:00执行数据更新
0 17 * * 1-5 cd /path/to/DWAD && python scripts/download_data.py >> logs/daily_update.log 2>&1
```

## 注意事项

### 1. 网络稳定性

全量更新需要下载大量数据，请确保：
- 网络连接稳定
- API token有效
- 未触发掘金API的访问频率限制

### 2. 磁盘空间

全量更新会临时需要额外的磁盘空间：
- 旧数据删除前需要空间：~500MB
- 新数据下载需要空间：~500MB
- 建议预留：>2GB可用空间

### 3. 执行时间

全量更新的执行时间取决于：
- 股票数量：~5000只A股
- 时间跨度：从起始日期到今天
- API响应速度
- 网络速度

预计耗时：**10-30分钟**

### 4. 错误处理

如果更新过程中出现错误：

1. **查看日志**：检查 `logs/` 目录下的日志文件
2. **检查网络**：确认能访问掘金API
3. **验证token**：确保 `config/config.yaml` 中的token有效
4. **重新执行**：系统会自动重试失败的股票

### 5. 数据验证

更新完成后，建议验证数据：

```python
from dwad.data_storage.parquet_storage import ParquetStorage

storage = ParquetStorage()

# 获取统计信息
stats = storage.get_storage_stats()
print(f"总股票数: {stats['total_stocks']}")
print(f"最后更新: {stats['last_update']}")

# 检查某只股票的数据
df = storage.load_stock_data('SHSE.600000')
print(f"数据条数: {len(df)}")
print(f"日期范围: {df['date'].min()} 到 {df['date'].max()}")
```

## 技术实现

### 核心方法

**`DataDownloader.full_refresh_data()`**

```python
def full_refresh_data(self) -> bool:
    """
    全量刷新数据：删除所有现有股票数据，重新下载全部历史数据
    
    Returns:
        是否更新成功
    """
    # 1. 获取已存储的股票列表
    existing_symbols = self.storage.list_available_stocks()
    
    # 2. 分批处理
    for batch_symbols in batches:
        for symbol in batch_symbols:
            # 3. 删除旧数据
            self.storage.delete_stock_data(symbol)
            
            # 4. 下载新数据
            data = self.fetcher.get_historical_data(
                symbol=symbol,
                start_date=start_date,
                end_date=end_date
            )
            
            # 5. 保存新数据
            self.storage.save_stock_data(symbol, data)
    
    return success
```

### 存储方法

**`ParquetStorage.delete_stock_data()`**

```python
def delete_stock_data(self, symbol: str) -> bool:
    """
    删除股票历史数据文件
    
    Args:
        symbol: 股票代码
        
    Returns:
        是否删除成功
    """
    file_path = self._get_stock_file_path(symbol)
    if file_path.exists():
        file_path.unlink()
    return True
```

## 与增量更新的对比

### 增量更新（已弃用）

```python
# 旧方法：只下载新增日期的数据
start_date = last_date + 1天
new_data = fetch(start_date, today)
append_to_existing(new_data)  # ❌ 新旧数据复权基准不一致
```

**问题**：新旧数据使用不同的复权基准，导致数据不连续

### 全量更新（当前方法）

```python
# 新方法：删除旧数据，重新下载全部
delete_all_data()
new_data = fetch(start_date, today)  # ✅ 所有数据使用统一复权基准
save_data(new_data)
```

**优势**：所有数据使用当前时刻的统一复权基准，确保连续性

## 常见问题

### Q1: 每天都要重新下载所有数据吗？

**A**: 是的。为了确保前复权数据的连续性，必须使用统一的复权基准。

### Q2: 能不能只更新最近一段时间的数据？

**A**: 不建议。部分更新无法保证数据连续性，可能导致指数计算错误。

### Q3: 更新时间太长怎么办？

**A**: 可以考虑：
- 减少股票数量（只保留关注的股票池）
- 缩短历史时间跨度（调整 `default_start_date`）
- 在网络条件好的时段执行

### Q4: 会不会浪费资源？

**A**: 相比数据准确性，全量更新的成本是值得的。而且：
- API调用量在合理范围内
- 磁盘空间成本极低
- 时间成本可接受（自动化执行）

### Q5: 如何验证数据连续性？

**A**: 检查价格变化率：
```python
df = storage.load_stock_data('SHSE.600000')
df['pct_change'] = df['close_price'].pct_change()

# 查找异常涨跌幅（>15%可能是数据问题）
abnormal = df[abs(df['pct_change']) > 0.15]
print(abnormal[['date', 'close_price', 'pct_change']])
```

## 更新日志

### 2025-10-21
- ✅ 实现全量刷新数据功能 `full_refresh_data()`
- ✅ 添加删除股票数据方法 `delete_stock_data()`
- ✅ 修改默认更新策略为全量更新
- ✅ 标记增量更新为已弃用
- ✅ 更新文档说明

## 相关文档

- [系统架构设计](../系统架构设计.md)
- [掘金API知识文档](../掘金API知识文档.md)
- [指数计算异常处理说明](./指数计算异常处理说明.md)
